{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714052334171,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"stUHtyXwtUTc"},"outputs":[],"source":["# used for manipulating directory paths\n","import os\n","\n","# Scientific and vector computation for python\n","import numpy as np\n","\n","# Plotting library\n","from matplotlib import pyplot\n","\n","# Optimization module in scipy\n","from scipy import optimize\n","\n","# tells matplotlib to embed plots within the notebook\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":657,"status":"ok","timestamp":1714053824835,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"iPKROwkOtpkn","outputId":"8b94cac3-551c-4842-a894-8b9d26038638"},"outputs":[{"name":"stdout","output_type":"stream","text":["(178,)\n","(150, 13)\n","(150,)\n","(28, 13)\n","(28,)\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n","[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"]}],"source":["# datos de entrenamiento almacenados en los arreglos X, y\n","data = np.loadtxt(\"wine_preparado.csv\", delimiter=',')\n","\n","# seleccionamos las columnas para X y y\n","X, y = data[:, 1:], data[:,0]\n","\n","# cada elemento(e) del arreglo y se convierte en entero\n","y = np.array([int(e) for e in y])\n","print(y.shape)\n","# elimina su segunda dimesion ejemplo (n,1) se elimina 1\n","y = np.squeeze(y)\n","\n","# reorganizamos al columna de y\n","y[y == 1] = 0\n","y[y == 2] = 1\n","y[y == 3] = 2\n","\n","# Seleccionamos los valores de las caracteristicaa de prueba\n","Xp= X[150:,:]\n","# Seleccionamos los valores de las caracteristicaa de entrenamiento\n","X = X[:150,:]\n","\n","# Seleccionamos los valores de y de prueba\n","yp = y[150:]\n","# Seleccionamos los valores de y de entrenamiento\n","y = y[:150]\n","\n","# imprimimos las dimensiones de entrenamiento\n","print(X.shape)\n","print(y.shape)\n","# imprimimos las dimensiones de prueba\n","print(Xp.shape)\n","print(yp.shape)\n","\n","# imprimimos todos los valores de y de entrenamiento\n","print(y)\n","# imprimimos todos los valores de y de prueba\n","print(yp)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"YN2IfP_5vz8v","outputId":"ed1b63c6-ce2a-47d0-982b-7e49c74d776d"},"outputs":[{"name":"stdout","output_type":"stream","text":["(140,)\n","(33,)\n","(173,)\n"]}],"source":["# Configurando parametros necesario\n","input_layer_size  = 13  # Entrada de 13 caracteristicas (columnas de X)\n","hidden_layer_size = 10   # 10 unidades ocultas ((columna de caracteristicas+1 + num_labels)/2)\n","num_labels = 3          # 3 etiquetas, de 0 a 2 (valores de y)\n","\n","# creamos el diccionario pesos\n","pesos = {}\n","# inicializamos theta1 y theta2 creando una matriz de valores randomicos de 10x14 y 3x11 respectivamente\n","pesos['Theta1'] = np.random.rand(hidden_layer_size, input_layer_size+1)\n","pesos['Theta2'] = np.random.rand(num_labels, hidden_layer_size+1)\n","\n","#asigna los valores de pesos[theta1 y 2] en theta1 y theta2\n","Theta1, Theta2 = pesos['Theta1'], pesos['Theta2']\n","\n","# la funcion ravel transforma una matriz multidimensional en una matriz unidimensional\n","print(Theta1.ravel().shape)\n","print(Theta2.ravel().shape)\n","\n","# inicializamos nn_params uniendo ambas matrices unidimenionales en una sola columna\n","nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n","print(nn_params.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"IYkAveQh0e87"},"outputs":[],"source":["# funcion sigmoide\n","# pone cada valor del dataset en un rango de 0 a 1, como una probabilidad, por ejemplo, de cuan positiva o deseable sea esa caracteristica del vino\n","def sigmoid(z):\n","\n","    return 1.0 / (1.0 + np.exp(-z))\n","\n","# funcion del gradiente de la funcion sigmoide\n","# calcular los gradientes de los pesos en las capas ocultas\n","# ayuda a ajustar los pesos de manera que la red neuronal pueda\n","# aprender de manera efectiva a partir de los datos de entrenamiento\n","def sigmoidGradient(z):\n","\n","    g = np.zeros(z.shape)\n","\n","    g = sigmoid(z) * (1 - sigmoid(z))\n","\n","    return g"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"Aevzq-rt0vKn"},"outputs":[],"source":["def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_= 1.0):\n","\n","    # Remodele nn_params nuevamente en los parámetros Theta1 y Theta2, las matrices\n","    # de peso para nuestra red neuronal de 2 capas.\n","    # Se toma una parte del vector de parámetros nn_params correspondiente a los\n","    # pesos entre la capa de entrada y la capa oculta.\n","    #Estos pesos se remodelan en una matriz Theta1 con hidden_layer_size filas y (input_layer_size + 1) columnas.\n","    #La dimensión (input_layer_size + 1) surge porque cada neurona en la capa oculta tiene un peso adicional\n","    # correspondiente al término de sesgo.\n","    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n","                        (hidden_layer_size, (input_layer_size + 1)))# 10, 14\n","    # Se toma la parte restante del vector de parámetros nn_params correspondiente\n","    # a los pesos entre la capa oculta y la capa de salida.\n","    # Estos pesos se remodelan en una matriz Theta2 con num_labels filas (el número de etiquetas de salida)\n","    # y (hidden_layer_size + 1) columnas.\n","    #La dimensión (hidden_layer_size + 1) surge por la misma razón que en Theta1, para incluir el término\n","    # de sesgo en cada neurona de la capa oculta.\n","    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n","                        (num_labels, (hidden_layer_size + 1))) #3,11\n","\n","    m = y.size\n","\n","    J = 0\n","    \n","    # Estas líneas de código inicializan las matrices Theta1_grad y Theta2_grad con ceros. Estas matrices\n","    # se utilizarán para almacenar los gradientes de la función de costo con respecto a los pesos Theta1 y Theta2\n","    Theta1_grad = np.zeros(Theta1.shape)\n","    Theta2_grad = np.zeros(Theta2.shape)\n","    \n","    #capa de entrada\n","    #columna de 1(sesgo)\n","    #son parámetros adicionales en una red neuronal que permiten realizar ajustes en la salida de cada neurona\n","    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n","\n","    #capa oculta\n","    #Se calculan las activaciones de la capa oculta aplicando la función sigmoide a la combinación lineal de las\n","    #activaciones de la capa de entrada (a1) y los pesos de la capa oculta (Theta1).\n","    a2 = sigmoid(a1.dot(Theta1.T))\n","    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n","\n","    #capa de salida\n","    #Se calculan las activaciones de la capa de salida aplicando la función sigmoide a la combinación lineal\n","    #de las activaciones de la capa oculta (a2) y los pesos de la capa de salida (Theta2).\n","    a3 = sigmoid(a2.dot(Theta2.T))\n","\n","    #la funcion reshape convierte la matriz y en un vector unidimensional\n","    #se aplanará la matriz en un vector unidimensional mientras conserva el mismo número total de elementos.\n","    y_matrix = y.reshape(-1)\n","\n","    #la funcion eye crea una matriz del tamaño de num_labels x num_labels\n","    y_matrix = np.eye(num_labels)[y_matrix]\n","\n","    temp1 = Theta1\n","    temp2 = Theta2\n","\n","    # Agregar el termino de regularización\n","\n","    #formula de regularizacion\n","    #la regularizacion evita el sobreajuste, que se obtiene al ajustarse demasiado solamente a los datos de entrenamiento\n","    #Este término adicional penaliza los modelos que son demasiado complejos, es decir, que tienen coeficientes de pesos demasiado grandes\n","    reg_term = (lambda_ / (2 * m)) * (np.sum(np.square(temp1[:, 1:])) + np.sum(np.square(temp2[:, 1:])))\n","\n","    #funcion de costo\n","    J = (-1 / m) * np.sum((np.log(a3) * y_matrix) + np.log(1 - a3) * (1 - y_matrix)) + reg_term\n","\n","    # Backpropogation\n","    # para calcular los gradientes de la funcion de costo\n","\n","    #Cálculo del error de la capa de salida\n","    #Se calcula el error de la capa de salida delta_3 como la diferencia entre las activaciones\n","    # de la capa de salida (a3) y las etiquetas reales (y_matrix)\n","    delta_3 = a3 - y_matrix\n","    #Cálculo del error de la capa oculta\n","    #Se calcula el error de la capa oculta delta_2 propagando hacia atrás el error de la capa de salida (delta_3\n","    delta_2 = delta_3.dot(Theta2)[:, 1:] * sigmoidGradient(a1.dot(Theta1.T))\n","\n","    #Cálculo de los gradientes de los pesos\n","    #Se calculan los gradientes de los pesos de la red neuronal utilizando los errores de las capas de salida y oculta.\n","    Delta1 = delta_2.T.dot(a1)\n","    Delta2 = delta_3.T.dot(a2)\n","\n","    # Agregar regularización al gradiente\n","\n","    # Regularización del gradiente para Theta1\n","    # Primero, se calcula el gradiente de los pesos entre la capa de entrada y la capa oculta \n","    Theta1_grad = (1 / m) * Delta1\n","    # se aplica regularización al gradiente. Para hacer esto, se ajustan los valores de los gradientes de los pesos,\n","    # excepto para los sesgos (es decir, para todas las columnas excepto la primera), sumando el término de regularización\n","    # a cada elemento.\n","    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n","\n","    # Regularización del gradiente para Theta2\n","    Theta2_grad = (1 / m) * Delta2\n","    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n","\n","    # Concatenación de los gradientes\n","    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n","\n","    return J, grad"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"k4TjzE2h3BqL","outputId":"4cc70294-8e0a-4454-8ee2-d5da822b7686"},"outputs":[{"name":"stdout","output_type":"stream","text":["Costo en parametros: 11.912688 \n"]}],"source":["# En este caso, lambda_ se establece en 0, lo que significa que no se aplica regularización al \n","lambda_ = 1\n","# El valor de J se asigna a la variable J, mientras que el valor _ se utiliza para almacenar cualquier otro\n","# resultado devuelto por nnCostFunction que no sea relevante para el propósito actual.\n","J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n","print('Costo en parametros: %.6f ' % J)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"EnKgJRZq-x3U"},"outputs":[],"source":["def randInitializeWeights(L_in, L_out, epsilon_init=0.2):\n","    \"\"\"\n","    Inicializa aleatoriamente los pesos de una capa en una red neuronal.\n","\n","    Parameters\n","    ----------\n","    L_in : int\n","        Número de conexiones entrantes.\n","\n","    L_out : int\n","        Número de conexiones salientes.\n","\n","    epsilon_init : float, optional\n","        Rango de valores que puede tomar el peso de un uniforme\n","        distribución.\n","\n","    Returns\n","    -------\n","    W : array_like\n","        El peso se inicializó a valores aleatorios. Tenga en cuenta\n","        que W debe establecerse en una matriz de tamaño (L_out, 1 + L'In)\n","        ya que la primera columna de W maneja los términos de \"sesgo\".\"\"\"\"\"\n","\n","    # Se crea una matriz de ceros de tamaño (L_out, 1 + L_in), donde L_out es\n","    # el número de neuronas en la capa de salida y L_in es el número de neuronas en la capa de entrada.\n","    W = np.zeros((L_out, 1 + L_in))\n","    # Se sobrescribe la matriz de ceros W con valores aleatorios en el rango [0, 1)\n","    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n","\n","    return W"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"znk_8rO0-6fE","outputId":"9b9a683b-1174-4635-e633-f26c03d371ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Inicialización de parámetros de redes neuronales...\n"]}],"source":["print('Inicialización de parámetros de redes neuronales...')\n","\n","initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n","initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n","\n","# Desenrrollr parametros\n","initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"-ysYL_hX_D0k","outputId":"a0ae7984-4221-4297-c093-bbd6bdecef90"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\hlope\\AppData\\Local\\Temp\\ipykernel_5848\\1595555740.py:14: OptimizeWarning: Unknown solver options: maxiter\n","  res = optimize.minimize(costFunction,\n","C:\\Users\\hlope\\AppData\\Local\\Temp\\ipykernel_5848\\1332050153.py:5: RuntimeWarning: overflow encountered in exp\n","  return 1.0 / (1.0 + np.exp(-z))\n"]}],"source":["# Una vez que haya completado la tarea, cambie el maxiter a un valor mayor para ver cómo ayuda más capacitación.\n","options= {'maxiter': 5000}\n","\n","#  También deberías probar diferentes valores de lambda.\n","lambda_ = 1\n","\n","# Crear \"short hand\" para minimizar la función de costos.\n","costFunction = lambda p: nnCostFunction(p, input_layer_size,\n","                                        hidden_layer_size,\n","                                        num_labels, X, y, lambda_)\n","\n","# Ahora, costFunction es una función que toma solo un argumento.\n","# (los parámetros de la red neuronal)\n","res = optimize.minimize(costFunction,\n","                        initial_nn_params,\n","                        jac=True,\n","                        method='TNC',\n","                        options=options)\n","\n","# obtener la solución de la optimización\n","nn_params = res.x\n","\n","# Obtenga Theta1 y Theta2 de nn_params\n","Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n","                    (hidden_layer_size, (input_layer_size + 1)))\n","\n","Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n","                    (num_labels, (hidden_layer_size + 1)))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"XDnQrQM4_0Ct"},"outputs":[],"source":["def predict(Theta1, Theta2, X):\n","    \"\"\"\n","    Predecir la etiqueta de una entrada dada una red neuronal entrenada\n","    Genera la etiqueta predicha de X dados los pesos entrenados de un red neuronal (Theta1, Theta2)\n","    \"\"\"\n","    # Valores útiles\n","    m = X.shape[0]\n","    num_labels = Theta2.shape[0]\n","\n","    # Necesitas devolver las siguientes variables correctamente\n","    p = np.zeros(m)\n","    h1 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))\n","    h2 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), h1], axis=1), Theta2.T))\n","    p = np.argmax(h2, axis=1)\n","    return p"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"rxMinI1Y_6AG","outputId":"1a129f5b-574a-468a-eae5-dee890bd06e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n","Training Set Accuracy: 99.333333\n"]}],"source":["pred = predict(Theta1, Theta2, X[:,:])\n","print(pred)\n","print('Training Set Accuracy: %f' % (np.mean(pred == y[:]) * 100))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714052374183,"user":{"displayName":"Carlos Walter Pacheco Lora","userId":"05889892519883337793"},"user_tz":240},"id":"1Qrmea3i_hip","outputId":"aeae320f-f8da-4288-8e9e-f7e9ad65133a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n","Training Set Accuracy: 100.000000\n"]}],"source":["pred = predict(Theta1, Theta2, Xp[:,:])\n","print(pred)\n","print('Training Set Accuracy: %f' % (np.mean(pred == yp[:]) * 100))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOnY6s1mk+XoVNHHBXQBN/G","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
